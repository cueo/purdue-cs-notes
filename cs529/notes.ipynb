{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Surface of ML\n",
    "### Taxonomy\n",
    "* Knowledge\n",
    "  * Black\n",
    "  * Gray\n",
    "  * White\n",
    "* Target\n",
    "  * Training\n",
    "  * Testing\n",
    "* Goals\n",
    "  * Confidence reduction\n",
    "  * Misclassification\n",
    "\n",
    "### Poisoning Attacks\n",
    "* Manipulate training dataset\n",
    "\n",
    "### Evasion Attacks\n",
    "* Manipulate input samples at test time to cause misclassification\n",
    "\n",
    "### Evasion Attacks\n",
    "* Discover the parameters of the model\n",
    "\n",
    "### Membership Inference\n",
    "* Infer if a data is part of training dataset or of the same distribution as training data\n",
    "\n",
    "### Security Goals\n",
    "\n",
    "| Goal                  | Attack               |\n",
    "|-----------------------|----------------------|\n",
    "| Data integrity        | Poisoning attack     |\n",
    "| Model integrity       | Evasion attack       |\n",
    "| Model confidentiality | Model extraction     |\n",
    "| Privacy               | Membership inference |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Examples and Evasion Attacks\n",
    "\n",
    "### Adversarial Example\n",
    "* Input to model to cause the model to make a mistake\n",
    "* Add noise, features to fool the model\n",
    "\n",
    "### Attacking a Network\n",
    "* **Goal 1:** find an input that is not Y (say, iguana) but will be classified as Y\n",
    "$$L(\\hat{y}, y) = \\dfrac{1}{2} (\\hat{y} - y_{iguana})^2$$\n",
    "\n",
    "* **Goal 2:** find an input that is Y (say, cat) but will be classified as Y' (say, iguana)\n",
    "$$L(\\hat{y}, y) = \\dfrac{1}{2} (\\hat{y} - y_{iguana})^2 + \\lambda (x - xcat)^2$$\n",
    "\n",
    "### Fast-Gradient Sign Method (FGSM)\n",
    "* Perturb the image so that it is misclassified but still looks like the original\n",
    "$$adv\\_x = x + \\epsilon * sign(\\nabla_x J(\\theta, x, y))$$\n",
    "\n",
    "* How to change the objective function to misclassify?\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "$$\\begin{aligned}\n",
    "w &= \\begin{bmatrix}\n",
    "1 & 3 & -1 & 2 & 2 & 3\n",
    "\\end{bmatrix}\\\\\n",
    "X &= \\begin{bmatrix}\n",
    "1\\\\\n",
    "-1\\\\\n",
    "2\\\\\n",
    "3\\\\\n",
    "-2\n",
    "\\end{bmatrix}\\\\\n",
    "\\hat{y} &= w^Tx + b\\\\\n",
    "&= -4\n",
    "\\end{aligned}$$\n",
    "\n",
    "* How to change $X \\rightarrow X^*$ radically but $X^* \\subseteq X$?\n",
    "$$\\begin{aligned}\n",
    "\\dfrac{\\delta y}{\\delta x} &= w^T\\\\\n",
    "X &= X + \\epsilon w^T\\\\\n",
    "&= \\begin{bmatrix}\n",
    "1\\\\\n",
    "-1\\\\\n",
    "2\\\\\n",
    "3\\\\\n",
    "-2\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "0.2 * 1\\\\\n",
    "0.2 * 3\\\\\n",
    "\\dots\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "1.2\\\\\n",
    "-0.4\\\\\n",
    "\\dots\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Thus,\n",
    "$$\\begin{aligned}\n",
    "\\hat{y} &= 1.6\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Box Attacks and Transferability\n",
    "* Steps\n",
    "  1. Query the remote ML model using some API with inputs to obtain their labels\n",
    "  2. Use this labeled data to create local surrogate ML model\n",
    "  3. Use local model to craft adversarial example which are misclassified by the remote model\n",
    "\n",
    "### Transferability\n",
    "* Ability of an attack crafted against a surrogate local model to be effective against an unknown model\n",
    "\n",
    "#### Intra-technique Transferability\n",
    "* Models A and B are trained using the same ML technique\n",
    "\n",
    "#### Cross-technique Transferability\n",
    "* Models A and B are different\n",
    "\n",
    "#### Results\n",
    "* Cell $(i, j)$ represents percentage of adversarial samples produced using model $i$ misclassified by model $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network (GAN)\n",
    "* Comprised of two neural networks - **Discriminator** and **Generator**\n",
    "  * Both compete against each other\n",
    "* **Goal**: Given training data, generate new samples from the same distribution, ie: learn $p_{model}(x)$ similar to $p_{data}(x)$\n",
    "\n",
    "### Models\n",
    "<img src=\"./pictures/gan_models.png\" alt=\"Discriminator and Generator\" width=\"800\"/>\n",
    "\n",
    "#### Discriminative Model\n",
    "* Model that classifies data into two categories - fake or not\n",
    "\n",
    "#### Generative Model\n",
    "* Model pre-trained on some distribution $D$ when given some random distribution $Z$ produces a distribution $D'$ which is close to $D$\n",
    "\n",
    "### Math:\n",
    "##### Binary Cross Entropy Loss\n",
    "$$L(y, \\hat{y} = [y \\log \\hat{y} + (1-y) \\log (1 - \\hat{y})]$$\n",
    "\n",
    "#### Discriminator\n",
    "* Data from $p_{data}(x)$:\n",
    "$$\\begin{aligned}\n",
    "y &= 1 \\text{ // true label}\\\\\n",
    "\\hat{y} &= D(x) \\text{ // output of the discriminator model}\\\\\n",
    "L(\\hat{y}, y) &= L(D(x), 1) = \\log (D(x))\n",
    "\\end{aligned}$$\n",
    "\n",
    "* Data from Generator:\n",
    "$$\\begin{aligned}\n",
    "y &= 0 \\text{ // true label}\\\\\n",
    "\\hat{y} &= D(G(z)) \\text{ // output of the discriminator model}\\\\\n",
    "L(\\hat{y}, y) &= L(D(x), 1) = \\log (1 - D(G(z)))\n",
    "\\end{aligned}$$\n",
    "\n",
    "<img src=\"./pictures/discriminator_loss1.png\" alt=\"Discriminator Loss, y=1\" width=\"500\"/>\n",
    "<img src=\"./pictures/discriminator_loss2.png\" alt=\"Discriminator Loss, y=2\" width=\"500\"/>\n",
    "<!-- ![Discriminator Loss, y=1](./pictures/discriminator_loss1.png)\n",
    "![Discriminator Loss, y=0](./pictures/discriminator_loss2.png) -->\n",
    "\n",
    "* Objective:\n",
    "$$\\max [\\log (D(x)) + \\log (1 - D(G(z)))]$$\n",
    "\n",
    "* Why max?\n",
    "Look at the graphs, when:\n",
    "  * $y = 1$ and $D(x) = 1$, loss is 0\n",
    "  * $y = 1$ and $D(x) = 0$, loss is $-\\infty$\n",
    "  * $y = 0$ and $D(x) = 0$, loss is 0\n",
    "  * $y = 0$ and $D(x) = 1$, loss is $-\\infty$\n",
    "Thus, we need to maximize loss.\n",
    "\n",
    "\n",
    "#### Generator\n",
    "$$\\begin{aligned}\n",
    "y &= 0 \\text{ // because fake image}\\\\\n",
    "\\hat{y} &= D(G(z)) \\text{ // output of Discriminator}\\\\\n",
    "L(\\hat{y}, y) &= L(D(G(z)), 0) = \\log (1 - D(G(z)))\n",
    "\\end{aligned}$$\n",
    "\n",
    "<img src=\"./pictures/generator_loss.png\" alt=\"Generator Loss, y=0\" width=\"500\"/>\n",
    "<!-- ![Generator Loss, y=0](./pictures/generator_loss.png) -->\n",
    "\n",
    "We add notation $\\log D(x)$ just so that we can combine the two:\n",
    "$$\\min [\\log (D(x)) + \\log (1 - D(G(z)))]$$\n",
    "\n",
    "Combining the two, we have:\n",
    "$$\\min_{G} \\max_{D} [\\log (D(x)) + \\log (1 - D(G(z)))]$$\n",
    "\n",
    "For all samples:\n",
    "$$\\min_{G} \\max_{D} \\dfrac{1}{m} \\sum_{i=1}^{m} [\\log (D(x)) + \\log (1 - D(G(z)))]$$\n",
    "\n",
    "## Momentum\n",
    "$$\\begin{aligned}\n",
    "\\end{aligned}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('assignment3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c9f439cdf62a9ebb07dc2af8d7bf220d77eb850c07d6f9df42203b0fbcb22d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
