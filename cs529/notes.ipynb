{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Surface of ML\n",
    "### Taxonomy\n",
    "* Knowledge\n",
    "  * Black\n",
    "  * Gray\n",
    "  * White\n",
    "* Target\n",
    "  * Training\n",
    "  * Testing\n",
    "* Goals\n",
    "  * Confidence reduction\n",
    "  * Misclassification\n",
    "\n",
    "### Poisoning Attacks\n",
    "* Manipulate training dataset\n",
    "\n",
    "### Evasion Attacks\n",
    "* Manipulate input samples at test time to cause misclassification\n",
    "\n",
    "### Evasion Attacks\n",
    "* Discover the parameters of the model\n",
    "\n",
    "### Membership Inference\n",
    "* Infer if a data is part of training dataset or of the same distribution as training data\n",
    "\n",
    "### Security Goals\n",
    "\n",
    "| Goal                  | Attack               |\n",
    "|-----------------------|----------------------|\n",
    "| Data integrity        | Poisoning attack     |\n",
    "| Model integrity       | Evasion attack       |\n",
    "| Model confidentiality | Model extraction     |\n",
    "| Privacy               | Membership inference |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Adversarial Examples and Evasion Attacks\n",
    "\n",
    "### Adversarial Example\n",
    "* Input to model to cause the model to make a mistake\n",
    "* Add noise, features to fool the model\n",
    "\n",
    "### Attacking a Network\n",
    "#### Targeted Attacks\n",
    "* Can incorportate label information into the attack\n",
    "\n",
    "##### Goal 1\n",
    "* Find an input that is not $Y$ (say, iguana) but will be classified as $Y$\n",
    "$$L(y, \\hat{y}) = \\dfrac{1}{2}(y-y_{iguana})^2$$\n",
    "\n",
    "##### Goal 2\n",
    "* Find an input that is $Y$ (say, cat) but will be classified as $Y'$ (say, iguana)\n",
    "$$L(y, \\hat{y}) = \\dfrac{1}{2}(y-y_{iguana})^2 + \\lambda (x - x_{cat})^2$$\n",
    "\n",
    "#### Non-Targeted Attack\n",
    "* No control or information about the class labels\n",
    "\n",
    "##### Fast-Gradient Sign Method (FGSM)\n",
    "* Perturb the image so that it is misclassified but still looks like the original\n",
    "$$adv\\_x = x + \\epsilon * sign(\\nabla L)$$\n",
    "\n",
    "* $\\epsilon =$ *adversarial pertubration*\n",
    "\n",
    "* Gradient descent wrt the weights (parameters) to decrease the loss\n",
    "$$\\theta' = \\theta - \\alpha \\nabla_{\\theta} L(x, y, \\theta)$$\n",
    "\n",
    "* Gradient ascent wrt the input to increase the loss\n",
    "$$x' = x + \\alpha \\nabla_{\\theta} L(x, y, \\theta)$$\n",
    "\n",
    "* Steps:\n",
    "  1. Model setup\n",
    "  2. Train the model\n",
    "  3. Generate pertubrations\n",
    "  4. Run the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('assignment3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c9f439cdf62a9ebb07dc2af8d7bf220d77eb850c07d6f9df42203b0fbcb22d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
