{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Example Detection\n",
    "\n",
    "### Attribute-Steered Model\n",
    "#### Attribute witness extraction\n",
    "* Intersection of __attribute substitution__ and __attribute preservation__.\n",
    "* New model is created by:\n",
    "  * __Neuron weakening:__ weaken the non-witness neurons\n",
    "  * __Neuron strengthening:__ strengthen the witness neurons\n",
    "\n",
    "* Detection (??)\n",
    "  * what does false positive on benign input mean?\n",
    "  * incorrect classification to a class, is it classification with trigger?\n",
    "\n",
    "### Neural network invariant checking\n",
    "* Allow correct behaviors and forbit malicious behaviors (eg: assert certain _behaviors_)\n",
    "* __Value invariants:__ possible value distribution for each neuron\n",
    "* __Provenance invariants:__ possible delta between the values [_pattern_] of two layers of neurons\n",
    "* Examples (??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Sample Defenses\n",
    "### Gradient Masking\n",
    "* Hide or destroy the gradient (so that all gradient based attacks fail)\n",
    "* Defense techniques:\n",
    "  * __Distillation defense:__ changes the scale of last hidden layer\n",
    "  * __Input preprocessing:__ transforms input images by resizing, cropping, discretizing pixels\n",
    "  * __Defense GAN:__ uses GAN to transform perturbed images into clean images\n",
    "\n",
    "#### Evading gradient masking\n",
    "* Approximate gradients\n",
    "* Hiding or breaking gradients makes the loss surface zig-zaggy, when doing backward pass replace function with difficult gradient with one that has nice gradient\n",
    "\n",
    "### Certified Adversarial Robustness\n",
    "* Model gives prediction and a certificate that the prediction is constant (holds) within an $l2$ around the input\n",
    "* Randomized smoothing leads to smoother decision boundaries\n",
    "  * Smooth ùëì into a new classifier $g$ (the ‚Äúsmoothed classifier‚Äù) as follows:\n",
    "$$g(x) = \\text{the most probable prediction by } f \\text{ of random Gaussian corruptions of } x$$\n",
    "* Large random perturbations ‚Äúdrown out‚Äù small adversarial perturbations (??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidentiality and Privacy Threats in ML\n",
    "### Model Inversion\n",
    "* Extra sensitive inputs by leverage knowledge about model structure and some information about an individual or object\n",
    "\n",
    "#### Type 1\n",
    "* For example, if one of the features is sensitive, attacker can judge the feature by setting it to 0 or 1 and checking the output label, $y$\n",
    "* What if $y$ does not change on changing feature?\n",
    "* White box or black box? White - attacker has information about parameters or black - attacker changes one of those but cannot see other parameters.\n",
    "* Formally:\n",
    "  * infer $x_n$ given $f$, $x_1, x_2, ..., x_{n-1}$ and $y$ (??) where $x_n \\in \\{v_1, v_2, ..., v_s\\}$\n",
    "  * compute $y_j = f(x_1, x_2, ..., x_{n-1}, v_j)$ for each $j$\n",
    "  * output $v_j$ that maximizes:\n",
    "  $$dist(y, y_j) \\times P(v_j | x_1, x_2, ..., x_{n-1})$$\n",
    "  * what is $y$? (??)\n",
    "\n",
    "#### Type 2\n",
    "* Given $f$ and $y$, infer $X$\n",
    "* Use gradient descent to search for input $X$ which maximizes probability of $y$\n",
    "* White box or black box?\n",
    "\n",
    "### Model Extraction\n",
    "* Learn a close approximation of the model $f$ using as few queries to the model as possible\n",
    "* For example, logistic regression function can be converted to a linear equation in $n+1$ variables\n",
    "* __Extraction attack:__ learn model architecture or parameters\n",
    "* __Oracle attack:__ construct a substitute model\n",
    "\n",
    "### Membership Inference\n",
    "* Given an input $x$ and a black box access to the model $f$, determine if $x \\in D$, meaning whether $x$ was part of the training data (or distribution (??))\n",
    "* Privacy concern?\n",
    "  * If $x$ is used for training a medical model, if one can determine $x \\in D$, one can predict whether an individual have health issue or not\n",
    "\n",
    "#### Attack stages\n",
    "1. Development of shadow dataset\n",
    "  * Goal: develop a dataset $D'$ which closely emulates the original dataset $D$\n",
    "  * Techniques: statistics-based, query-based, active learning, region-based\n",
    "2. Generation of attack model training dataset\n",
    "  * Takes input from shadown dataset $D'$ as $(x', y')$ and outputs a probability vector $p = (p_1, p_2, ..., p_k)$ and a binary label indicating \"in\" or \"out\"\n",
    "  * Partition $D'$ into $D_1, D_2, ..., D_s$\n",
    "  * $\\forall j,$ train $f_j$ to output \"in\" for $D_j$ and \"out\" for $D \\setminus D_j$\n",
    "  * Obtain attack training data, $p = (p_1, p_2, ..., p_k)$ and label \"in\" or \"out\" \n",
    "3. Training and depoloyment of membership inference attack model\n",
    "  * Given input of probability vector, return \"in\" or \"out\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
