{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Example Detection\n",
    "\n",
    "### Attribute-Steered Model\n",
    "#### Attribute witness extraction\n",
    "* Intersection of __attribute substitution__ and __attribute preservation__.\n",
    "* New model is created by:\n",
    "  * __Neuron weakening:__ weaken the non-witness neurons\n",
    "  * __Neuron strengthening:__ strengthen the witness neurons\n",
    "\n",
    "* Detection (??)\n",
    "  * what does false positive on benign input mean?\n",
    "  * incorrect classification to a class, is it classification with trigger?\n",
    "\n",
    "### Neural network invariant checking\n",
    "* Allow correct behaviors and forbit malicious behaviors (eg: assert certain _behaviors_)\n",
    "* __Value invariants:__ possible value distribution for each neuron\n",
    "* __Provenance invariants:__ possible delta between the values [_pattern_] of two layers of neurons\n",
    "* Examples (??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Sample Defenses\n",
    "### Gradient Masking\n",
    "* Hide or destroy the gradient (so that all gradient based attacks fail)\n",
    "* Defense techniques:\n",
    "  * __Distillation defense:__ changes the scale of last hidden layer\n",
    "  * __Input preprocessing:__ transforms input images by resizing, cropping, discretizing pixels\n",
    "  * __Defense GAN:__ uses GAN to transform perturbed images into clean images\n",
    "\n",
    "#### Evading gradient masking\n",
    "* Approximate gradients\n",
    "* Hiding or breaking gradients makes the loss surface zig-zaggy, when doing backward pass replace function with difficult gradient with one that has nice gradient\n",
    "\n",
    "### Certified Adversarial Robustness\n",
    "* Model gives prediction and a certificate that the prediction is constant (holds) within an $l2$ around the input\n",
    "* Randomized smoothing leads to smoother decision boundaries\n",
    "  * Smooth ùëì into a new classifier $g$ (the ‚Äúsmoothed classifier‚Äù) as follows:\n",
    "$$g(x) = \\text{the most probable prediction by } f \\text{ of random Gaussian corruptions of } x$$\n",
    "* Large random perturbations ‚Äúdrown out‚Äù small adversarial perturbations (??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidentiality and Privacy Threats in ML\n",
    "### Model Inversion\n",
    "* Extra sensitive inputs by leverage knowledge about model structure and some information about an individual or object\n",
    "\n",
    "#### Type 1\n",
    "* For example, if one of the features is sensitive, attacker can judge the feature by setting it to 0 or 1 and checking the output label, $y$\n",
    "* What if $y$ does not change on changing feature?\n",
    "* White box or black box? White - attacker has information about parameters or black - attacker changes one of those but cannot see other parameters.\n",
    "* Formally:\n",
    "  * infer $x_n$ given $f$, $x_1, x_2, ..., x_{n-1}$ and $y$ (??) where $x_n \\in \\{v_1, v_2, ..., v_s\\}$\n",
    "  * compute $y_j = f(x_1, x_2, ..., x_{n-1}, v_j)$ for each $j$\n",
    "  * output $v_j$ that maximizes:\n",
    "  $$dist(y, y_j) \\times P(v_j | x_1, x_2, ..., x_{n-1})$$\n",
    "  * what is $y$? (??)\n",
    "\n",
    "#### Type 2\n",
    "* Given $f$ and $y$, infer $X$\n",
    "* Use gradient descent to search for input $X$ which maximizes probability of $y$\n",
    "* White box or black box?\n",
    "\n",
    "### Model Extraction\n",
    "* Learn a close approximation of the model $f$ using as few queries to the model as possible\n",
    "* For example, logistic regression function can be converted to a linear equation in $n+1$ variables\n",
    "* __Extraction attack:__ learn model architecture or parameters\n",
    "* __Oracle attack:__ construct a substitute model\n",
    "\n",
    "### Membership Inference\n",
    "* Given an input $x$ and a black box access to the model $f$, determine if $x \\in D$, meaning whether $x$ was part of the training data (or distribution (??))\n",
    "* Privacy concern?\n",
    "  * If $x$ is used for training a medical model, if one can determine $x \\in D$, one can predict whether an individual have health issue or not\n",
    "\n",
    "#### Attack stages\n",
    "1. Development of shadow dataset\n",
    "  * Goal: develop a dataset $D'$ which closely emulates the original dataset $D$\n",
    "  * Techniques: statistics-based, query-based, active learning, region-based\n",
    "2. Generation of attack model training dataset\n",
    "  * Takes input from shadown dataset $D'$ as $(x', y')$ and outputs a probability vector $p = (p_1, p_2, ..., p_k)$ and a binary label indicating \"in\" or \"out\"\n",
    "  * Partition $D'$ into $D_1, D_2, ..., D_s$\n",
    "  * $\\forall j,$ train $f_j$ to output \"in\" for $D_j$ and \"out\" for $D \\setminus D_j$\n",
    "  * Obtain attack training data, $p = (p_1, p_2, ..., p_k)$ and label \"in\" or \"out\" \n",
    "3. Training and depoloyment of membership inference attack model\n",
    "  * Given input of probability vector, return \"in\" or \"out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Privacy\n",
    "* Guarantees:\n",
    "  * Raw data will not be viewed\n",
    "  * Output will have distortions\n",
    "\n",
    "### Confidence Interval\n",
    "* Range of values for which we are fairly sure (say, $x%$) the true value lies in\n",
    "$$\\overline{X} \\pm Z\\dfrac{s}{\\sqrt{n}}$$\n",
    "\n",
    "### Standard DP\n",
    "* Analysts sends a query to a software called _DP guard_\n",
    "* Guard sends the query to the DB or model to retrieve the output\n",
    "* Guard adds __noise__ to the output (in order to protect the confidentiality of the individual whose data was accessed from DB) and sends back the response to the analyst\n",
    "\n",
    "### Local DP\n",
    "* User anonymizes the data themselves and send to the aggregator\n",
    "* Aggregator doesn't have access to the real data\n",
    "\n",
    "#### Advantages / disadvantages\n",
    "* Local DP less prone to data leak as the aggregator does not have access to real data\n",
    "* Might be less accurate (?)\n",
    "\n",
    "### Formalism\n",
    "* Whether or not more data is adding into $D$, both the results with or without will be the same, $R$. $A = 1$ is ideal in which case both the results are identical, whereas if $A$ is much larger or smaller, the result deviates too much.\n",
    "$$\\dfrac{P(Q(D_I)) = R}{P(Q(D_{I \\pm i})) = R} \\leq A$$\n",
    "\n",
    "* Putting, $A = e^\\epsilon$:\n",
    "$$\\dfrac{P(R | D_I}{P(R | D_{I \\pm i}) = R} \\leq e^\\epsilon$$\n",
    "\n",
    "#### Global sensitivity\n",
    "* $F(D) = X$ is a deterministic, non-privatized function over dataset $D$ which returns $X$, a vector $k$ real numbers.\n",
    "* Global sensitivity is the sum of the worst case differences between datasets $D1$ and $D2$ differing by at most one element, $\\Delta F$:\n",
    "$$\\Delta F = \\max_{D1, D2} \\left|\\left| F(D1) - F(D2) \\right|\\right|_{L1}$$\n",
    "\n",
    "#### Noise adding mechanism\n",
    "* Privatizing by adding noise from Laplace distribution:\n",
    "$$P(R = x | D \\text{ is true world}) = \\dfrac{\\epsilon}{2 \\Delta F} \\exp{-\\dfrac{\\left| x - F(D) \\right| \\epsilon}{\\Delta F}}$$\n",
    "* Laplace ($\\epsilon-$ differential)\n",
    "$$F(x) = f(x) + Lap\\left(\\dfrac{s}{\\epsilon}\\right)$$\n",
    "* Exponential ($\\epsilon-$ differential)\n",
    "  * Works with both numeric and categorical data\n",
    "  * Releases the identity of the element with MAX noisy score and not that of the score itself (??)\n",
    "* Gaussian ($\\epsilon, \\delta-$ differential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
