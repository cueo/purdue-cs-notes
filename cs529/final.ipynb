{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Security\n",
    "* __Risk__\n",
    "  * a function of __threats__ exploiting __vulnerabilities__ to damage or obtain unauthorized access to __assets__\n",
    "  * potential for loss or damage to __asset__ as a result of a __threat__ exploiting a __vulnearibility__\n",
    "* __Assets__\n",
    "  * what we are trying to protect\n",
    "  * software or hardware\n",
    "  * contain information or support information related activities\n",
    "* __Vulnerabilities__\n",
    "  * weakness in system that could be exploited or triggered by a threat\n",
    "  * Source:\n",
    "    * bad software / hardware\n",
    "    * bad design\n",
    "    * bad policy / configuration\n",
    "    * system misuse\n",
    "* __Threat__\n",
    "  * specific means by which an attacker can put a system at risk\n",
    "* __Attack__\n",
    "  * when someone attempts to exploit a vulnerability\n",
    "  * Kinds:\n",
    "    * Passive (eavesdropping, keylogger)\n",
    "    * Active (password guessing)\n",
    "    * DoS or DDoS\n",
    "* __Compromise__\n",
    "  * when an attack is successful\n",
    "\n",
    "### Security Goals / Triads\n",
    "* Confidentiality\n",
    "  * assets are _accessed_ (viewing, printing or simply knowing its existence) only by authorized parties\n",
    "* Integrity\n",
    "  * assets are _modified_ only by authorized parties\n",
    "* Availability\n",
    "  * degree to which system is accessible and in functioning condition\n",
    "\n",
    "* Privacy\n",
    "  * individual's desire to control who has access to their personal data\n",
    "\n",
    "### Threat Assessment and Model\n",
    "* Threat assessment\n",
    "  * what kind of threats\n",
    "  * capabilities of the adversary\n",
    "  * limitations of the adversary\n",
    "* Threat model\n",
    "  * result of threat assessment\n",
    "  * characterization of the threats a system might face\n",
    "\n",
    "* Evaluation of security\n",
    "  * Identify the security goals\n",
    "    * what assets need protection\n",
    "  * Perform a threat assessmnet\n",
    "  * Security analysis\n",
    "    * any feasible attacks that can violate security goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Surface of ML\n",
    "### Taxonomy\n",
    "* Knowledge\n",
    "  * Black\n",
    "  * Gray\n",
    "  * White\n",
    "* Target\n",
    "  * Training\n",
    "  * Testing\n",
    "* Goals\n",
    "  * Confidence reduction\n",
    "  * Misclassification\n",
    "\n",
    "### Poisoning Attacks\n",
    "* Manipulate training dataset\n",
    "* Decision boundary is changed\n",
    "\n",
    "### Evasion Attacks\n",
    "* Manipulate input samples at test time to cause misclassification\n",
    "* Decision boundary does not change but the input is changed\n",
    "\n",
    "### Model Extraction\n",
    "* Discover the parameters of the model\n",
    "\n",
    "### Membership Inference\n",
    "* Infer if a data is part of training dataset or of the same distribution as training data\n",
    "\n",
    "### Security Goals\n",
    "\n",
    "| Goal                  | Attack               |\n",
    "|-----------------------|----------------------|\n",
    "| Data integrity        | Poisoning attack     |\n",
    "| Model integrity       | Evasion attack       |\n",
    "| Model confidentiality | Model extraction     |\n",
    "| Privacy               | Membership inference |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Examples and Evasion Attacks\n",
    "\n",
    "### Adversarial Example\n",
    "* Input to model to cause the model to make a mistake\n",
    "* Add noise, features to fool the model\n",
    "\n",
    "### Evasion Attacks\n",
    "* **Goal 1:** find an input that is not Y (say, iguana) but will be classified as Y\n",
    "$$\\begin{align*}\n",
    "L(\\hat{y}, y) &= \\dfrac{1}{2} (\\hat{y} - y_{iguana})^2\\\\\n",
    "x &= x - \\alpha \\dfrac{\\partial L}{\\partial x}\n",
    "\\end{align*}$$\n",
    "\n",
    "* **Goal 2:** find an input that is Y (say, cat) but will be classified as Y' (say, iguana)\n",
    "$$\\begin{align*}\n",
    "L(\\hat{y}, y) &= \\dfrac{1}{2} (\\hat{y} - y_{iguana})^2 + \\lambda (x - x_{cat})^2\\\\\n",
    "x &= x - \\alpha \\left( \\dfrac{\\partial L}{\\partial x} + \\lambda (x - x_{cat}) \\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "### Fast-Gradient Sign Method (FGSM)\n",
    "* Perturb the image so that it is misclassified but still looks like the original\n",
    "$$adv\\_x = x + \\epsilon * sign(\\nabla_x J(\\theta, x, y))$$\n",
    "\n",
    "* How to change the objective function to misclassify?\n",
    "???\n",
    "\n",
    "#### Exercise\n",
    "$$\\begin{aligned}\n",
    "w &= \\begin{bmatrix}\n",
    "1 & 3 & -1 & 2 & 2 & 3\n",
    "\\end{bmatrix}\\\\\n",
    "X &= \\begin{bmatrix}\n",
    "1\\\\\n",
    "-1\\\\\n",
    "2\\\\\n",
    "3\\\\\n",
    "-2\n",
    "\\end{bmatrix}\\\\\n",
    "\\hat{y} &= w^Tx + b\\\\\n",
    "&= -4\n",
    "\\end{aligned}$$\n",
    "\n",
    "* How to change $X \\rightarrow X^*$ radically but $X^* \\subseteq X$?\n",
    "$$\\begin{aligned}\n",
    "\\dfrac{\\delta y}{\\delta x} &= w^T\\\\\n",
    "X &= X + \\epsilon w^T\\\\\n",
    "&= \\begin{bmatrix}\n",
    "1\\\\\n",
    "-1\\\\\n",
    "2\\\\\n",
    "3\\\\\n",
    "-2\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "0.2 * 1\\\\\n",
    "0.2 * 3\\\\\n",
    "\\dots\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "1.2\\\\\n",
    "-0.4\\\\\n",
    "\\dots\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Thus,\n",
    "$$\\begin{aligned}\n",
    "\\hat{y} &= 1.6\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Box Attacks and Transferability\n",
    "* Steps\n",
    "  1. Query the remote ML model using some API with inputs to obtain their labels\n",
    "  2. Use this labeled data to create local surrogate ML model\n",
    "  3. Use local model to craft adversarial example which are misclassified by the remote model\n",
    "\n",
    "### Transferability\n",
    "* Ability of an attack crafted against a surrogate local model to be effective against an unknown model\n",
    "\n",
    "#### Intra-technique Transferability\n",
    "* Models A and B are trained using the same ML technique\n",
    "\n",
    "#### Cross-technique Transferability\n",
    "* Models A and B are different\n",
    "\n",
    "#### Results\n",
    "* Cell $(i, j)$ represents percentage of adversarial samples produced using model $i$ misclassified by model $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network (GAN)\n",
    "* Comprised of two neural networks - **Discriminator** and **Generator**\n",
    "  * Both compete against each other\n",
    "* **Goal**: Given training data, generate new samples from the same distribution, ie: learn $p_{model}(x)$ similar to $p_{data}(x)$\n",
    "\n",
    "### Models\n",
    "<img src=\"./pictures/gan_models.png\" alt=\"Discriminator and Generator\" width=\"800\"/>\n",
    "\n",
    "#### Discriminative Model\n",
    "* Model that classifies data into two categories - fake or not\n",
    "\n",
    "#### Generative Model\n",
    "* Model pre-trained on some distribution $D$ when given some random distribution $Z$ produces a distribution $D'$ which is close to $D$\n",
    "\n",
    "### Math:\n",
    "##### Binary Cross Entropy Loss\n",
    "$$L(y, \\hat{y} = [y \\log \\hat{y} + (1-y) \\log (1 - \\hat{y})]$$\n",
    "\n",
    "#### Discriminator\n",
    "* Data from $p_{data}(x)$:\n",
    "$$\\begin{aligned}\n",
    "y &= 1 \\text{ // true label}\\\\\n",
    "\\hat{y} &= D(x) \\text{ // output of the discriminator model}\\\\\n",
    "L(\\hat{y}, y) &= L(D(x), 1) = \\log (D(x))\n",
    "\\end{aligned}$$\n",
    "\n",
    "* Data from Generator:\n",
    "$$\\begin{aligned}\n",
    "y &= 0 \\text{ // true label}\\\\\n",
    "\\hat{y} &= D(G(z)) \\text{ // output of the discriminator model}\\\\\n",
    "L(\\hat{y}, y) &= L(D(x), 1) = \\log (1 - D(G(z)))\n",
    "\\end{aligned}$$\n",
    "\n",
    "<img src=\"./pictures/discriminator_loss1.png\" alt=\"Discriminator Loss, y=1\" width=\"500\"/>\n",
    "<img src=\"./pictures/discriminator_loss2.png\" alt=\"Discriminator Loss, y=2\" width=\"500\"/>\n",
    "<!-- ![Discriminator Loss, y=1](./pictures/discriminator_loss1.png)\n",
    "![Discriminator Loss, y=0](./pictures/discriminator_loss2.png) -->\n",
    "\n",
    "* Objective:\n",
    "$$\\max [\\log (D(x)) + \\log (1 - D(G(z)))]$$\n",
    "\n",
    "* Why max?\n",
    "Look at the graphs, when:\n",
    "  * $y = 1$ and $D(x) = 1$, loss is 0\n",
    "  * $y = 1$ and $D(x) = 0$, loss is $-\\infty$\n",
    "  * $y = 0$ and $D(x) = 0$, loss is 0\n",
    "  * $y = 0$ and $D(x) = 1$, loss is $-\\infty$\n",
    "Thus, we need to maximize loss.\n",
    "\n",
    "\n",
    "#### Generator\n",
    "$$\\begin{aligned}\n",
    "y &= 0 \\text{ // because fake image}\\\\\n",
    "\\hat{y} &= D(G(z)) \\text{ // output of Discriminator}\\\\\n",
    "L(\\hat{y}, y) &= L(D(G(z)), 0) = \\log (1 - D(G(z)))\n",
    "\\end{aligned}$$\n",
    "\n",
    "<img src=\"./pictures/generator_loss.png\" alt=\"Generator Loss, y=0\" width=\"500\"/>\n",
    "<!-- ![Generator Loss, y=0](./pictures/generator_loss.png) -->\n",
    "\n",
    "We add notation $\\log D(x)$ just so that we can combine the two:\n",
    "$$\\min [\\log (D(x)) + \\log (1 - D(G(z)))]$$\n",
    "\n",
    "Combining the two, we have:\n",
    "$$\\min_{G} \\max_{D} [\\log (D(x)) + \\log (1 - D(G(z)))]$$\n",
    "\n",
    "For all samples:\n",
    "$$\\min_{G} \\max_{D} \\dfrac{1}{m} \\sum_{i=1}^{m} [\\log (D(x)) + \\log (1 - D(G(z)))]$$\n",
    "\n",
    "### Issues\n",
    "* Vanishing gradient\n",
    "* Mode collapse (generator always produces same output)\n",
    "* Nash equilibrium - both models achieve convergence concurrently\n",
    "* Counting\n",
    "* Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Example Detection\n",
    "\n",
    "### Attribute-Steered Model\n",
    "#### Attribute witness extraction\n",
    "* Intersection of __attribute substitution__ and __attribute preservation__.\n",
    "* New model is created by:\n",
    "  * __Neuron weakening:__ weaken the non-witness neurons\n",
    "  * __Neuron strengthening:__ strengthen the witness neurons\n",
    "\n",
    "* Detection (??)\n",
    "  * what does false positive on benign input mean?\n",
    "  * incorrect classification to a class, is it classification with trigger?\n",
    "\n",
    "### Neural network invariant checking\n",
    "* Allow correct behaviors and forbit malicious behaviors (eg: assert certain _behaviors_)\n",
    "* __Value invariants:__ possible value distribution for each neuron\n",
    "* __Provenance invariants:__ possible delta between the values [_pattern_] of two layers of neurons\n",
    "* Examples (??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Sample Defenses\n",
    "### Gradient Masking\n",
    "* Hide or destroy the gradient (so that all gradient based attacks fail)\n",
    "* Defense techniques:\n",
    "  * __Distillation defense:__ changes the scale of last hidden layer\n",
    "  * __Input preprocessing:__ transforms input images by resizing, cropping, discretizing pixels\n",
    "  * __Defense GAN:__ uses GAN to transform perturbed images into clean images\n",
    "\n",
    "#### Evading gradient masking\n",
    "* Approximate gradients\n",
    "* Hiding or breaking gradients makes the loss surface zig-zaggy, when doing backward pass replace function with difficult gradient with one that has nice gradient\n",
    "\n",
    "### Certified Adversarial Robustness\n",
    "* Model gives prediction and a certificate that the prediction is constant (holds) within an $l2$ around the input\n",
    "* Randomized smoothing leads to smoother decision boundaries\n",
    "  * Smooth 𝑓 into a new classifier $g$ (the “smoothed classifier”) as follows:\n",
    "$$g(x) = \\text{the most probable prediction by } f \\text{ of random Gaussian corruptions of } x$$\n",
    "* Large random perturbations “drown out” small adversarial perturbations (??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidentiality and Privacy Threats in ML\n",
    "### Model Inversion\n",
    "* Extra sensitive inputs by leverage knowledge about model structure and some information about an individual or object\n",
    "\n",
    "#### Type 1\n",
    "* For example, if one of the features is sensitive, attacker can judge the feature by setting it to 0 or 1 and checking the output label, $y$\n",
    "* What if $y$ does not change on changing feature?\n",
    "* White box or black box? White - attacker has information about parameters or black - attacker changes one of those but cannot see other parameters.\n",
    "* Formally:\n",
    "  * infer $x_n$ given $f$, $x_1, x_2, ..., x_{n-1}$ and $y$ (??) where $x_n \\in \\{v_1, v_2, ..., v_s\\}$\n",
    "  * compute $y_j = f(x_1, x_2, ..., x_{n-1}, v_j)$ for each $j$\n",
    "  * output $v_j$ that maximizes:\n",
    "  $$dist(y, y_j) \\times P(v_j | x_1, x_2, ..., x_{n-1})$$\n",
    "  * what is $y$? (??)\n",
    "\n",
    "#### Type 2\n",
    "* Given $f$ and $y$, infer $X$\n",
    "* Use gradient descent to search for input $X$ which maximizes probability of $y$\n",
    "* White box or black box?\n",
    "\n",
    "### Model Extraction\n",
    "* Learn a close approximation of the model $f$ using as few queries to the model as possible\n",
    "* For example, logistic regression function can be converted to a linear equation in $n+1$ variables\n",
    "* __Extraction attack:__ learn model architecture or parameters\n",
    "* __Oracle attack:__ construct a substitute model\n",
    "\n",
    "### Membership Inference\n",
    "* Given an input $x$ and a black box access to the model $f$, determine if $x \\in D$, meaning whether $x$ was part of the training data (or distribution (??))\n",
    "* Privacy concern?\n",
    "  * If $x$ is used for training a medical model, if one can determine $x \\in D$, one can predict whether an individual have health issue or not\n",
    "\n",
    "#### Attack stages\n",
    "1. Development of shadow dataset\n",
    "  * Goal: develop a dataset $D'$ which closely emulates the original dataset $D$\n",
    "  * Techniques: statistics-based, query-based, active learning, region-based\n",
    "2. Generation of attack model training dataset\n",
    "  * Takes input from shadown dataset $D'$ as $(x', y')$ and outputs a probability vector $p = (p_1, p_2, ..., p_k)$ and a binary label indicating \"in\" or \"out\"\n",
    "  * Partition $D'$ into $D_1, D_2, ..., D_s$\n",
    "  * $\\forall j,$ train $f_j$ to output \"in\" for $D_j$ and \"out\" for $D \\setminus D_j$\n",
    "  * Obtain attack training data, $p = (p_1, p_2, ..., p_k)$ and label \"in\" or \"out\" \n",
    "3. Training and depoloyment of membership inference attack model\n",
    "  * Given input of probability vector, return \"in\" or \"out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Privacy\n",
    "* Guarantees:\n",
    "  * Raw data will not be viewed\n",
    "  * Output will have distortions\n",
    "\n",
    "### Confidence Interval\n",
    "* Range of values for which we are fairly sure (say, $x%$) the true value lies in\n",
    "$$\\overline{X} \\pm Z\\dfrac{s}{\\sqrt{n}}$$\n",
    "\n",
    "### Standard DP\n",
    "* Analysts sends a query to a software called _DP guard_\n",
    "* Guard sends the query to the DB or model to retrieve the output\n",
    "* Guard adds __noise__ to the output (in order to protect the confidentiality of the individual whose data was accessed from DB) and sends back the response to the analyst\n",
    "\n",
    "### Local DP\n",
    "* User anonymizes the data themselves and send to the aggregator\n",
    "* Aggregator doesn't have access to the real data\n",
    "\n",
    "#### Advantages / disadvantages\n",
    "* Local DP less prone to data leak as the aggregator does not have access to real data\n",
    "* Might be less accurate (?)\n",
    "\n",
    "### Formalism\n",
    "* Whether or not more data is adding into $D$, both the results with or without will be the same, $R$. $A = 1$ is ideal in which case both the results are identical, whereas if $A$ is much larger or smaller, the result deviates too much.\n",
    "$$\\dfrac{P(Q(D_I)) = R}{P(Q(D_{I \\pm i})) = R} \\leq A$$\n",
    "\n",
    "* Putting, $A = e^\\epsilon$:\n",
    "$$\\dfrac{P(R | D_I}{P(R | D_{I \\pm i}) = R} \\leq e^\\epsilon$$\n",
    "\n",
    "#### Global sensitivity\n",
    "* $F(D) = X$ is a deterministic, non-privatized function over dataset $D$ which returns $X$, a vector $k$ real numbers.\n",
    "* Global sensitivity is the sum of the worst case differences between datasets $D1$ and $D2$ differing by at most one element, $\\Delta F$:\n",
    "$$\\Delta F = \\max_{D1, D2} \\left|\\left| F(D1) - F(D2) \\right|\\right|_{L1}$$\n",
    "\n",
    "#### Noise adding mechanism\n",
    "* Privatizing by adding noise from Laplace distribution:\n",
    "$$P(R = x | D \\text{ is true world}) = \\dfrac{\\epsilon}{2 \\Delta F} \\exp{-\\dfrac{\\left| x - F(D) \\right| \\epsilon}{\\Delta F}}$$\n",
    "* Laplace ($\\epsilon-$ differential)\n",
    "$$F(x) = f(x) + Lap\\left(\\dfrac{s}{\\epsilon}\\right)$$\n",
    "* Exponential ($\\epsilon-$ differential)\n",
    "  * Works with both numeric and categorical data\n",
    "  * Releases the identity of the element with MAX noisy score and not that of the score itself (??)\n",
    "* Gaussian ($\\epsilon, \\delta-$ differential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
